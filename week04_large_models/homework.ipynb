{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKl2dqHxx9eVooNA2VFgNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da5472c281214cf5bd97fa38b476d8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_710f693325604d8cabeb3c17e79940b6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_71068694e7b94a66a9f152ada29870e6",
              "IPY_MODEL_98714743216849458471ab4758350240",
              "IPY_MODEL_aba1c0d2c8a7442db243b8046f0ea99f"
            ]
          }
        },
        "710f693325604d8cabeb3c17e79940b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71068694e7b94a66a9f152ada29870e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b6ef40ef59041dd98ea18c3debf5d54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7e122b6821c4dd8b819c4d8bba5142b"
          }
        },
        "98714743216849458471ab4758350240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d33367c7059645f5a78771ac8047c48f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5778fe60169b423981a78309337f64ca"
          }
        },
        "aba1c0d2c8a7442db243b8046f0ea99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d666e06bbf364b2fa9c45174c29aea44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 18.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99d72ded561d45b2b04f1c0918157a52"
          }
        },
        "5b6ef40ef59041dd98ea18c3debf5d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7e122b6821c4dd8b819c4d8bba5142b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d33367c7059645f5a78771ac8047c48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5778fe60169b423981a78309337f64ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d666e06bbf364b2fa9c45174c29aea44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99d72ded561d45b2b04f1c0918157a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "There is one main quest and several side-quests after that. To get the full grade, you need attain 10 or more points in whichever way you want.\n",
        "\n",
        "### Part 1: Memory-efficient training (5 pts)\n",
        "\n",
        "![img](https://steamuserimages-a.akamaihd.net/ugc/280721626864094662/C48355EE16889197B8D000A198F970CD7E64CB7A/?imw=512&imh=342&ima=fit&impolicy=Letterbox&imcolor=%23000000&letterbox=true)\n",
        "\n",
        "__Your quest__ is to fine-tune a GPT-2-Large by fitting it into 11GiB GPU memory (as in 1080Ti or 2080Ti). We deliberately limit GPU memory below and recommend you to check the peak memory usage via: [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "vMJ9Na9HMAK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEt3S33OL-y-",
        "outputId": "b06ca965-96b9-447a-b2ae-7ae60eee6d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting memory limit to 98.45%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "max_memory_gib = torch.cuda.get_device_properties('cuda').total_memory / 2 ** 30\n",
        "torch.cuda.set_per_process_memory_fraction(min(1.0, 11 / max_memory_gib))\n",
        "print(f\"Setting memory limit to {min(1.0, 11 / max_memory_gib) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 is a [popular language model by OpenAI](https://openai.com/blog/better-language-models/). We're gonna use [huggingface transformers](https://huggingface.co/docs/transformers/index) as a shortcut to access this model. Here's how it works:"
      ],
      "metadata": {
        "id": "2pXriyFUUDIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dependencies: !pip install transformers==4.16 datasets==1.18.3\n",
        "import transformers\n",
        "model_name = 'gpt2-large'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "sTuoIY_tNSVk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here's how the model works: tokenizer converts raw data to pytorch tensors\n",
        "batch = tokenizer([\"A cat sat\", \"import numpy\"], return_tensors='pt')\n",
        "print(\"Batch:\", repr(batch)[:70].replace('\\n', ' '), ' ...')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEYYK1clPm-_",
        "outputId": "8e63b609-3021-4552-dd37-56db187eb8a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: {'input_ids': tensor([[   32,  3797,  3332],         [11748,   299, 32  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model turns those tensors into logits (pre-softmax activations)\n",
        "pred = model(**batch)\n",
        "print(\"Logits shape:\", pred.logits.shape, \" -- [batch size, sequence_length, vocab size]\")\n",
        "\n",
        "# a model is also a standard PyTorch module that has .parameters, can be sent .to(device), etc\n",
        "print(f\"Parameters: {sum(param.numel() for param in model.parameters())/1e6:0.2f} million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZBZ5u3hRIos",
        "outputId": "6fac71b5-5e5c-4165-f514-1f58fd9d2501"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])  -- [batch size, sequence_length, vocab size]\n",
            "Parameters: 774.03 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fun fact: you can use the model to generate text given prefix\n",
        "generated_ids = model.generate(**batch, max_length=16)\n",
        "print(\"Sample A:\", tokenizer.decode(generated_ids[0]))\n",
        "print(\"Sample B:\", tokenizer.decode(generated_ids[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWYjNSsLOFX6",
        "outputId": "66d93537-b72a-44b4-d402-58619f420aba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample A: A cat sat on the edge of the bed, staring at the ceiling.\n",
            "\n",
            "Sample B: import numpy as np\n",
            "\n",
            "from scipy.stats import mean,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a sample data you can use for prototyping -- and demonstrating that your algorithm works."
      ],
      "metadata": {
        "id": "mLWpcLbWYoeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"wikitext\", \"wikitext-2-v1\")['train']\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "test_batch = tokenizer(sorted(data['text'], key=len)[-64:], max_length=1024, padding=True, pad_to_multiple_of=256, return_tensors='pt')\n",
        "\n",
        "# if you want actual training, consider this dataset: https://huggingface.co/datasets/transformersbook/codeparrot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "da5472c281214cf5bd97fa38b476d8e8",
            "710f693325604d8cabeb3c17e79940b6",
            "71068694e7b94a66a9f152ada29870e6",
            "98714743216849458471ab4758350240",
            "aba1c0d2c8a7442db243b8046f0ea99f",
            "5b6ef40ef59041dd98ea18c3debf5d54",
            "f7e122b6821c4dd8b819c4d8bba5142b",
            "d33367c7059645f5a78771ac8047c48f",
            "5778fe60169b423981a78309337f64ca",
            "d666e06bbf364b2fa9c45174c29aea44",
            "99d72ded561d45b2b04f1c0918157a52"
          ]
        },
        "id": "i0kndblQWVt9",
        "outputId": "77f58a58-7320-4946-f6d6-d251a7dc87fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da5472c281214cf5bd97fa38b476d8e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2268: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Victory conditions\n",
        "\n",
        "- __(1/5 points)__ it trains in some shape or form, regardless of batch size and optimizer\n",
        "- __(2/5 points)__ it trains with sequence length 1024, batch size 64, with any optimizer (e.g. SGD)\n",
        "- __(3/5 points)__ same, but using the Adam optimizer, and the training loss is demonstrated to decrease on a fixed batch\n",
        "- __(4/5 points)__ most of the computation (flops) should pe performed on GPU, the training performance is at least 0.5x as fast as simply running forward/backward on GPU (in terms of sequences per second)\n",
        "- __(5/5 points)__ __master parameters should be kept in full float32 precision__.\n",
        "\n",
        "\n",
        "__How do I do that?__ There's a bunch of things you could try: you can pick and choose which of them you employ. Below are some (but not all) options:\n",
        "- __Gradient accumulation / micro-batching:__ you probably can't process 64 sequences at once -- but what if you accumulate them over several forward/batckward passes with a smaller batch size.\n",
        "- __Gradient checkpointing:__ you can further reduce activation memory by not storing intermediate activations. You can learn how to usa built-in checkpoints [from their docs](https://huggingface.co/docs/transformers/main_classes/model) or build your own using [vanilla checkpointing](https://pytorch.org/docs/stable/checkpoint.html).\n",
        "- __Mixed precision:__ you can use [pytorch AMP](https://pytorch.org/docs/stable/amp.html) to cast activations to 16-bit. If that is not enough, you can cast (some) parameters to 16-bit as well. Victory conditions require that the optimized parameters must stay 32-bit, but no one said you must keep GPU parameters that way.\n",
        "- __Offloading:__ any tensor you don't need right now can be offloaded from GPU to RAM. This is especially true for optimizer statistics.\n",
        "- __Quantization:__ if everything else fails, you could reduce memory usage further via 8-bit quantization, [e.g. for Adam](https://github.com/facebookresearch/bitsandbytes). However, please make sure the loss still goes down after you're done blaspheming.\n",
        "\n",
        "\n",
        "\n",
        "__Off limits:__\n",
        "- using more than 11GiB of GPU memory at any point is forbidden (check with [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html))\n",
        "- using non-Adam optimizer (e.g. SGD) will only count for 2/5 points. __Adafactor is not Adam__. But FusedAdam or quantized Adam are both legit -- if you can show that the loss goes down;\n",
        "- finetuning must update all model parameters on every batch; fine-tuning only a subset of parameters will not get a full grade;\n",
        "- changing the model structure - your code must run the same computation as the original GPT-2-large up to a floating point precision. Removing layers can be useful in practice, but it's off limits for this task.\n",
        "- deepspeed and fairscale. There's a separate ~~cauldron~~ assignment for that."
      ],
      "metadata": {
        "id": "tQdgK3ohSyN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if it helps, <YOUR CODE HERE>"
      ],
      "metadata": {
        "id": "CR2KrOtGgz9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional assignments: pick at least one for full grade, or more if you're into it.\n",
        "\n",
        "### Pipeline parallelism (3+ points)\n",
        "\n",
        "__TL;DR__ implement pipeline parallelism using [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) or [`torch.rpc`](https://pytorch.org/docs/stable/rpc.html), your choice.\n",
        "\n",
        "![img](https://i.imgur.com/Va7GN6x.png)\n",
        "\n",
        "Please start from [PyTorch CIFAR](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) starter kit. We will not judge if you choose something heavier, e.g. ImageNet -- but that is not necessary.\n",
        "For this assignment, you will need to implement pipeline-parallel training similar to the one defined in [GPipe](https://arxiv.org/abs/1811.06965) (or lecture 4 notes). Feel free to use CPU or share one GPU for multiple processes. The main victory condition is that the model must show the same convergence rate as training on a single GPU -- which you will need to demosntrate.\n",
        "\n",
        "- __(1/3 points)__ doesn't have to be parallel, works with at least 2 stages\n",
        "- __(2/3 points)__ same parallelism as in GPipe, 4 or more stages, loss is shown to go down\n",
        "- __(3/3 points)__ achieve the same covnergence curves on CIFAR (or the task of your choice) as training with a single process\n",
        "- __(+1 bonus)__ demonstrate that your implementation can process a very large number of micro-batches (at least 1000 microbatches with 4 stages) without going OOM\n",
        "- __(+1 bonus)__ implement [pipedream](https://arxiv.org/pdf/1806.03377.pdf) or [interleaved pipeline](https://openreview.net/pdf?id=cw-EmNq5zfD) (this may be hard!)\n",
        "\n",
        "__Conditions:__ your implementation should use multiprocessing (i.e. not a single process sending data between devices). Existing pipelines (e.g. `torch.distributed.pipeline`) are off limits. If your code resembles one of existing pipelines too much, we will brutually interrogate you about the implementation details (as in \"we'll make sure it's easier to build your own\").\n",
        "\n",
        "### Tensor Parallelism (3+ points)\n",
        "_aka AlexNet-style parallelism_\n",
        "\n",
        "__TL;DR__ implement intra-layer model parallelism and make sure it works.\n",
        "\n",
        "Similarly to the pipeline task, we recommend that you start with the [PyTorch CIFAR](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) tutorial, but you can choose to use a more complex starter kit if you're feeling adventurous. If you don't have stable access to a multi-GPU setup, feel free to use CPU or share the same GPU across all processes.\n",
        "\n",
        "The main objective is to implement AlexNet-style model parallelism, wherein each device computes a subset of \"neurons\" in each layer and then exchanges results with other units. We recommend doing this with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) with either `all_reduce` or `all_gather`, depending on whether you split matrix rows or columns.\n",
        "\n",
        "- __(1/3 points)__ a simple architecture, `Sequential(linear1, relu, linear2)`,\n",
        "- __(2/3 points)__ a more complex architecture, add at least one batch- or layer normalization\n",
        "- __(3/3 points)__ train a model like this and show learning curves,\n",
        "- __(+1 bonus)__ parallelize either ResNet50 or some Transformer variant (__warning__, this is hard!),\n",
        "- __(+1 bonus)__ implement mixed column + row parallelism, run 2 dense layers with a single communication round (see below).\n",
        "\n",
        "For both options, please attach the learning curves of your model compared to regular single-process training. A decent explanation how this type of parallelism works can be found in Section 2.3 of [this paper](https://arxiv.org/pdf/2104.04473.pdf). Optimizations such as Figure 5 from that paper or this [weird trick](https://arxiv.org/abs/1404.5997) are welcome, but not required.\n",
        "\n",
        "\n",
        "### Bigger and badder (3+ points)\n",
        "\n",
        "__TL;DR__ repeat the first quest but for [GPT2-XL](https://huggingface.co/gpt2-xl) (~1.5B parameters) instead of GPT2-Large.\n",
        "\n",
        "![img](https://i.imgflip.com/48jwa8.png)\n",
        "\n",
        "- __(1/3 points)__ - do that with 16GiB GPU memory. If you can't get one through conventional means, try [free kaggle P100 quotas](https://www.kaggle.com/product-feedback/83643)\n",
        "- __(2/3 points)__ - the training should be at least 0.5x as fast as running forward/backward on GPU\n",
        "- __(3/3 points)__ - back to 11GiB memory and no more than 16GiB RAM\n",
        "- __(+1 bonus)__ - actually fine-tune the model on something (e.g. code, [codeparrot](https://huggingface.co/datasets/transformersbook/codeparrot)) and compare its generated outputs\n",
        "\n",
        "You will likely need to add more memory-saving tricks on top of what you used for GPT2-Large. It is okay to keep some parameters in fp16.\n",
        "Alternatively, __Deepspeed and fairscale are no longer forbidden__, but beware that they are not a free win -- you will have to tune the configuration for your specific setup.\n",
        "\n",
        "__Variant:__ if you somehow got your hands on a powerful GPU, we will also accept (A) [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) for 24-32GiB GPUs or multiple smaller ones (B) [T5-11B](https://huggingface.co/t5-11b) on any GPUs short of MI250X. Note that you still need to train the entire model: we will not accept solutions based on Denis' [8-bit + LoRA trick](https://huggingface.co/hivemind/gpt-j-6B-8bit).\n",
        "\n"
      ],
      "metadata": {
        "id": "7o7MWGmyhKIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <you guessed it...>"
      ],
      "metadata": {
        "id": "pCmlh97BhHtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
