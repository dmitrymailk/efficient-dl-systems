{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76efa72",
   "metadata": {},
   "source": [
    "# Week 5: home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b8117",
   "metadata": {},
   "source": [
    "## Assignment structure\n",
    "\n",
    "- DIY: loss scaling (3 points)\n",
    "- Efficient batching for language modelling (5 points)\n",
    "- Profiling of the pipeline (2 points)\n",
    "\n",
    "Your grade for the assignment is the sum of the points for the sections above. Maximum is 10 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b03ad",
   "metadata": {},
   "source": [
    "## DIY: loss scaling (1.5 + 1.5 points)\n",
    "\n",
    "Let's use a semantic segmentation pipeline for this section. Your task is to train the model in the AMP mode with loss scaler implemented by you. You **can use** `torch.cuda.amp.autocast` and you **cannot use** `torch.cuda.amp.GradScaler()` (you may only for checking your solution).\n",
    "\n",
    "Let us remind what loss scaling is. Loss scaling is used to avoid the gradient underflow problem, when computing gradients in FP16 precision. The issue here is that while training in full precision, we might acquire rather small values in the gradients, which will vanish when we cast a tensor to a half precision. To fix the problem the following solution is used:\n",
    "\n",
    "- make a forward pass for the model and compute the loss\n",
    "- multiply loss value to some factor\n",
    "- call `.backward()`\n",
    "- update model's master weights with **unscaled** FP32 gradients\n",
    "\n",
    "**Note.** Loss scaling might be done in two different ways: static and dynamic ones. In static mode, you choose a factor for scaling only once and use it for the whole training procedure. In dynamic mode you recompute the factor each time you scale the loss. \n",
    "\n",
    "For static scaling you will get **1.5 points**, for dynamic scaling you will get additional **1.5 points**. The task is done if you managed to stably achieve high accuracy values (0.985+) within 5 training epochs. For a start, you can run the training in a full precision mode, then try to run in an AMP mode with and without PyTorch loss scaler. You will observe that adding a scaler gives you additional accuracy points.\n",
    "\n",
    "**Hint.** To make sure that you're doing everything right, you might want to examine gradients' values: (almost) no zeros must be present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d63306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack data\n",
    "# !wget https://www.dropbox.com/s/tc1qo73rrm3gt3m/CARVANA.zip  # Carvana dataset\n",
    "# !unzip -q CARVANA.zip\n",
    "# !rm -rf ./train/.DS_Store\n",
    "# !rm -rf ./train_masks/.DS_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from carvana import Carvana\n",
    "from unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Carvana(\n",
    "    root=\".\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e64f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, epoch, num_epochs, device):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        # TODO: your code for loss scaling here\n",
    "        \n",
    "        accuracy = ((outputs > 0.5) == labels).float().mean()\n",
    "            \n",
    "        pbar.set_description(\n",
    "            f\"Loss: {round(loss.item(), 4)} \"\n",
    "            f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = Unet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(0, num_epochs):\n",
    "    train(train_loader, model, criterion, epoch, num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a4dde",
   "metadata": {},
   "source": [
    "## Efficient batching for language modelling (1 + 1 + 3 points)\n",
    "\n",
    "In this part we suggest you examine the efficiency of the three batching approaches we discussed during the seminar. Let us remind you shortly:\n",
    "\n",
    "**BRAIN**: pad everything to a fixed `max_length`\n",
    "\n",
    "**BIG BRAIN**: pad only in the `collate_fn`\n",
    "\n",
    "**ULTRA DUPER BIG BRAIN**: presort data to sample sequences smartly, preserving similar examples length in the batch\n",
    "\n",
    "___\n",
    "More formally, we suggest you download [WikiText-103 dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) and implement all of the mentioned approaches. Use the training part for all of the task's sub-problems.\n",
    "\n",
    "- For naive batching, you will need to implement a Pytorch Dataset class that will parse training data from the source files of the dataset and pad everything to a `max_length=640` of the training samples. For sequences longer than 640 tokens just truncate the overflowing part. **(1 point)**\n",
    "- For the second approach, you will need to implement the approach from the seminar for this dataset. More specifically, you needed to pad sequences only up to maximum sample length in the current batch. **(1 point)**\n",
    "- Finally, for the third approach, you will need to make a small trick. While initializing the dataset, you need to split it into the several bins (let's say, python lists) by samples length. For the task we suggest you uniformly split the samples list sorted by sample length. Conduct experiments for 1, 5, 10, 25, 50 bins. While calling a `__getitem__` method, you firstly sample a bin number, then sample the needed examples number form the bin and pad them with collator from the second subtask. **(3 points)**\n",
    "\n",
    "For each of the implemented methods mock one training epoch and provide min, max, mean and median batch processing times. Use a `pandas.DataFrame` to display the results in the notebook. For mocking a training epoch we suggest you construct a small GPT-2-like model: use `nn.Embedding` layer, `PositionalEncoding` class from `transformer.py` file and a single `nn.TransformerDecoder` layer with hidden size 1024 and 8 heads. For tokenization use `torchtext.data.utils.get_tokenizer(\"basic_english\")`. Run one epoch **without a backward pass**. Make sure you've [warmed up](https://forums.developer.nvidia.com/t/why-warm-up/48565) GPU before computing the statistics and do not forget about asynchronous CUDA kernels execution.\n",
    "\n",
    "**Note.** In the third subtask you might want to use (not obligatory) a `batch_sampler` in the data loader. For that, you need to inspect the corresponding Pytorch docs [section](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2746660",
   "metadata": {},
   "source": [
    "## Profiling (2 points)\n",
    "\n",
    "In this section, you're given a training script for a Transformer model on WikiText2 dataset. Your task is to examine the bottlenecks of the model. You can find the model script in the `transformer.py` file. As you might notice, this is a PyTorch Transformer implementation.\n",
    "\n",
    "We suppose that in this task you use [PyTorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html). However, feel free to use any other profiler that we've discussed. In the training function, you can vary the number of steps that are done during one epoch. We suggest you use only one epoch since our goal is not to train a model but to profile its performance.\n",
    "\n",
    "To complete the task, provide a detailed description of the model performance:\n",
    "- Forward pass\n",
    "    - Inspect PositionalEncoding layer\n",
    "    - Inspect the Embedding layer\n",
    "    - Inspect Attention layer (both self attention and projections computations)\n",
    "- Backward pass\n",
    "    - How long does it take compared to a forward pass?\n",
    "    \n",
    "Provide corresponding profiler's outputs and analyse them. We assume that you will analyse all of the mentioned model parts and other parts if you think it is reasonable (their time consumption is comparable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef310df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code sourse: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from transformer import generate_square_subsequent_mask, TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c248c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split=\"train\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    i = 0\n",
    "    for batch in trange(0, train_data.size(0) - 1, bptt, desc=\"Epoch progress: \"):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        # feel free to comment out this \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                  f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                  f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        i += 1\n",
    "            \n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "          f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
