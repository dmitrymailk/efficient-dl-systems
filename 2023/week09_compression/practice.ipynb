{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2341fbbf",
   "metadata": {},
   "source": [
    "# Week 9: Efficient model inference\n",
    "\n",
    "\n",
    "### Seminar outline\n",
    "1. Static PTQ\n",
    "    - Toy example\n",
    "    - MobileNetV2 on CIFAR10\n",
    "    - QAT for MobileNetV2\n",
    "    - Speed benchmark\n",
    "2. 42 GB T5 to a single GPU showcase\n",
    "    \n",
    "## Static PTQ\n",
    "### Toy example\n",
    "[Source](https://pytorch.org/docs/stable/quantization.html) of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e286587",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:18.755566Z",
     "end_time": "2023-03-14T00:51:19.742659Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.ao.quantization import DeQuantStub, QuantStub\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ca9d47",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:19.748732Z",
     "end_time": "2023-03-14T00:51:19.797645Z"
    }
   },
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 5, 3)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(4500, 100)\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        start = time()\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(self.flatten(x))\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [[\"conv\", \"relu\"]])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(4, 1, 32, 32)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7a750d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:19.803243Z",
     "end_time": "2023-03-14T00:51:19.811521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "M(\n  (quant): Quantize(scale=tensor([0.0534]), zero_point=tensor([64]), dtype=torch.quint8)\n  (conv): QuantizedConvReLU2d(1, 5, kernel_size=(3, 3), stride=(1, 1), scale=0.019489329308271408, zero_point=0)\n  (relu): Identity()\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear): QuantizedLinear(in_features=4500, out_features=100, scale=0.011786960065364838, zero_point=61, qscheme=torch.per_channel_affine)\n  (dequant): DeQuantize()\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e72365",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:19.812034Z",
     "end_time": "2023-03-14T00:51:23.744401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476 µs ± 6.54 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2892a818",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:23.744686Z",
     "end_time": "2023-03-14T00:51:26.353032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 µs ± 2.54 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = model_fp32(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aac44b",
   "metadata": {},
   "source": [
    "Why no speed up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b1ac85",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.350397Z",
     "end_time": "2023-03-14T00:51:26.387375Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model_int8.state_dict(), \"test_model_q.pth\")\n",
    "torch.save(model_fp32.state_dict(), \"test_model_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a38547",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.367753Z",
     "end_time": "2023-03-14T00:51:26.486651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 456379 Mar 13 23:51 test_model_q.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al test_model_q.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a2322e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.487789Z",
     "end_time": "2023-03-14T00:51:26.603118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 1802207 Mar 13 23:51 test_model_full.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al test_model_full.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c39638",
   "metadata": {},
   "source": [
    "### MobileNetV2 on CIFAR10\n",
    "[Source](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html) of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6320fe8e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.615065Z",
     "end_time": "2023-03-14T00:51:26.629423Z"
    }
   },
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            nn.ReLU(inplace=False),\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                # dw\n",
    "                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup, momentum=0.1),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=1000,\n",
    "        width_mult=1.0,\n",
    "        inverted_residual_setting=None,\n",
    "        round_nearest=8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if (\n",
    "            len(inverted_residual_setting) == 0\n",
    "            or len(inverted_residual_setting[0]) != 4\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"inverted_residual_setting should be non-empty \"\n",
    "                \"or a 4-element list, got {}\".format(inverted_residual_setting)\n",
    "            )\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(\n",
    "            last_channel * max(1.0, width_mult), round_nearest\n",
    "        )\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(\n",
    "                    block(input_channel, output_channel, stride, expand_ratio=t)\n",
    "                )\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.ao.quantization.fuse_modules(m, [\"0\", \"1\", \"2\"], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.ao.quantization.fuse_modules(\n",
    "                            m.conv, [str(idx), str(idx + 1)], inplace=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f830ed1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.636958Z",
     "end_time": "2023-03-14T00:51:26.638540Z"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print(\".\", end=\"\")\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = MobileNetV2()\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(\"cpu\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print(f'Size (MB): {os.path.getsize(\"temp.p\") / 1e6:.2f}')\n",
    "    os.remove(\"temp.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba416a35",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.642465Z",
     "end_time": "2023-03-14T00:51:26.644745Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders():\n",
    "    normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    dataset_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        download=True,\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size, sampler=train_sampler, num_workers=16\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size, sampler=test_sampler\n",
    "    )\n",
    "\n",
    "    return data_loader, data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e65d4a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.649885Z",
     "end_time": "2023-03-14T00:51:26.666518Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://download.pytorch.org/models/mobilenet_v2-b0353104.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a300b0e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.653646Z",
     "end_time": "2023-03-14T00:51:26.666797Z"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_dir = \"./\"\n",
    "float_model_file = \"mobilenet_v2-b0353104.pth\"\n",
    "scripted_float_model_file = \"mobilenet_quantization_scripted.pth\"\n",
    "scripted_quantized_model_file = \"mobilenet_quantization_scripted_quantized.pth\"\n",
    "\n",
    "train_batch_size = 512\n",
    "eval_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc546b59",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:26.662413Z",
     "end_time": "2023-03-14T00:51:28.700165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_loader, data_loader_test = prepare_data_loaders()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to(\"cpu\")\n",
    "\n",
    "# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n",
    "# while also improving numerical accuracy. While this can be used with any model, this is\n",
    "# especially common with quantized models.\n",
    "\n",
    "print(\"\\n Inverted Residual Block: Before fusion \\n\\n\", float_model.features[1].conv)\n",
    "float_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print(\"\\n Inverted Residual Block: After fusion\\n\\n\", float_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecf4421",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:28.703891Z",
     "end_time": "2023-03-14T00:51:28.708134Z"
    }
   },
   "outputs": [],
   "source": [
    "float_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2), nn.Linear(in_features=1280, out_features=10)\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(params=float_model.parameters())\n",
    "\n",
    "num_eval_batches = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5e0edd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:51:28.726084Z",
     "end_time": "2023-03-14T00:53:08.911387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35ce747c2be745a99074dcae46761c69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 63.23\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 73.98\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 76.18\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 78.29\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 80.00\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 81.40\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 80.75\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 81.42\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 80.65\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 81.29\n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(10):\n",
    "    float_model.train()\n",
    "    float_model.to(device)\n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = float_model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    float_model.eval()\n",
    "    top1, top5 = evaluate(\n",
    "        float_model,\n",
    "        criterion,\n",
    "        data_loader_test,\n",
    "        neval_batches=num_eval_batches,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\n",
    "        f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size)} images, {top1.avg:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65362d15",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:53:08.911724Z",
     "end_time": "2023-03-14T00:53:23.666134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 8.92\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 81.29\n"
     ]
    }
   ],
   "source": [
    "float_model.eval()\n",
    "float_model.cpu()\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(\n",
    "    float_model, criterion, data_loader_test, neval_batches=num_eval_batches\n",
    ")\n",
    "print(\n",
    "    f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size, )} images, {top1.avg:.2f}\"\n",
    ")\n",
    "torch.jit.save(\n",
    "    torch.jit.script(float_model), saved_model_dir + scripted_float_model_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e4db7",
   "metadata": {},
   "source": [
    "Let's quantize the model!\n",
    "\n",
    "Post-training static quantization involves not just converting the weights from float to int, as in dynamic quantization, but also performing the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting observer modules at different points that record this data). These distributions are then used to determine how the specifically the different activations should be quantized at inference time (a simple technique would be to simply divide the entire range of activations into 256 levels, but we support more sophisticated methods as well). Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats — and then back to ints — between every operation, resulting in a significant speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c4af71",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:53:16.560685Z",
     "end_time": "2023-03-14T00:53:43.208121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Inverted Residual Block:After observer insertion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n",
      "..................................................................................................Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.08577563613653183, zero_point=0, padding=(1, 1), groups=32)\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.10430050641298294, zero_point=63)\n",
      "  (2): Identity()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 2.36\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 77.93\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 512\n",
    "\n",
    "q_model = copy.deepcopy(float_model)\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "q_model.qconfig = torch.ao.quantization.default_qconfig\n",
    "print(q_model.qconfig)\n",
    "torch.ao.quantization.prepare(q_model, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print(\"Post Training Quantization Prepare: Inserting Observers\")\n",
    "print(\n",
    "    \"\\n Inverted Residual Block:After observer insertion \\n\\n\", q_model.features[1].conv\n",
    ")\n",
    "\n",
    "# Calibrate with the training set\n",
    "evaluate(q_model, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Post Training Quantization: Calibration done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.ao.quantization.convert(q_model, inplace=True)\n",
    "print(\"Post Training Quantization: Convert done\")\n",
    "print(\n",
    "    \"\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n\",\n",
    "    q_model.features[1].conv,\n",
    ")\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(q_model)\n",
    "\n",
    "top1, top5 = evaluate(\n",
    "    q_model, criterion, data_loader_test, neval_batches=num_eval_batches\n",
    ")\n",
    "print(\n",
    "    f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size, )} images, {top1.avg:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cae236",
   "metadata": {},
   "source": [
    "For this quantized model, we see lower accuracy on the eval dataset. This is because we used a simple min/max observer to determine quantization parameters. Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost a 4x decrease.\n",
    "\n",
    "In addition, we can significantly improve on the accuracy simply by using a different quantization configuration. We repeat the same exercise with the recommended configuration for quantizing for x86 architectures. This configuration does the following:\n",
    "\n",
    "Quantizes weights on a per-channel basis\n",
    "\n",
    "Uses a histogram observer that collects a histogram of activations and then picks quantization parameters in an optimal manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25174c1b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:53:43.208065Z",
     "end_time": "2023-03-14T00:54:52.062442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "...............................................................................................................................................................................................................................................................Evaluation accuracy on (64000,) images, 80.19\n",
      "Evaluation accuracy on (64000,) images, 80.19\n"
     ]
    }
   ],
   "source": [
    "per_channel_quantized_model = copy.deepcopy(float_model)\n",
    "per_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig(\n",
    "    \"fbgemm\"\n",
    ")\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
    "evaluate(per_channel_quantized_model, criterion, data_loader, num_calibration_batches)\n",
    "torch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\n",
    "top1, top5 = evaluate(\n",
    "    per_channel_quantized_model,\n",
    "    criterion,\n",
    "    data_loader_test,\n",
    "    neval_batches=num_eval_batches,\n",
    ")\n",
    "print(\n",
    "    f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size, )} images, {top1.avg:.2f}\"\n",
    ")\n",
    "torch.jit.save(\n",
    "    torch.jit.script(per_channel_quantized_model),\n",
    "    saved_model_dir + scripted_quantized_model_file,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size, )} images, {top1.avg:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd54d54",
   "metadata": {},
   "source": [
    "### QAT for MobileNetV2\n",
    "[Source](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html) for the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9466c98e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:54:52.064315Z",
     "end_time": "2023-03-14T00:54:52.067884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, criterion, optimizer, data_loader, device, ntrain_batches_log=200\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
    "    avgloss = AverageMeter(\"Loss\", \"1.5f\")\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time()\n",
    "        print(\".\", end=\"\")\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches_log:\n",
    "            print(\"Loss\", avgloss.avg)\n",
    "\n",
    "            print(f\"Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\")\n",
    "\n",
    "    print(f\"Full train set:  * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1768f1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:54:52.100445Z",
     "end_time": "2023-03-14T00:54:52.101230Z"
    }
   },
   "outputs": [],
   "source": [
    "qat_model = copy.deepcopy(float_model)\n",
    "qat_model.train()\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr=0.0001)\n",
    "qat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig(\"fbgemm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f6fbd13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:54:52.155196Z",
     "end_time": "2023-03-14T00:54:52.651251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# prepare_qat performs the “fake quantization”, preparing the model for quantization-aware training\n",
    "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print(\n",
    "    \"Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n\",\n",
    "    qat_model.features[1].conv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17621924",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-03-14T00:54:52.651009Z",
     "end_time": "2023-03-14T00:57:08.028966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................Full train set:  * Acc@1 88.008 Acc@5 99.502\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.69\n",
      "..................................................................................................Full train set:  * Acc@1 88.984 Acc@5 99.598\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.98\n",
      "..................................................................................................Full train set:  * Acc@1 89.328 Acc@5 99.622\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.78\n",
      "..................................................................................................Full train set:  * Acc@1 89.360 Acc@5 99.654\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.88\n",
      "..................................................................................................Full train set:  * Acc@1 89.526 Acc@5 99.628\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 83.01\n",
      "..................................................................................................Full train set:  * Acc@1 89.856 Acc@5 99.650\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.92\n",
      "..................................................................................................Full train set:  * Acc@1 90.028 Acc@5 99.648\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 82.99\n",
      "..................................................................................................Full train set:  * Acc@1 89.948 Acc@5 99.680\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 83.00\n",
      "..................................................................................................Full train set:  * Acc@1 89.952 Acc@5 99.632\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 83.05\n",
      "..................................................................................................Full train set:  * Acc@1 89.940 Acc@5 99.642\n",
      ".............................................................................................................................................................Evaluation accuracy on (64000,) images, 83.21\n"
     ]
    }
   ],
   "source": [
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(10):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, device=device)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.ao.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    quantized_model = torch.ao.quantization.convert(\n",
    "        qat_model.cpu().eval(), inplace=False\n",
    "    )\n",
    "    top1, top5 = evaluate(\n",
    "        quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches\n",
    "    )\n",
    "    print(\n",
    "        f\"Evaluation accuracy on {(num_eval_batches * eval_batch_size, )} images, {top1.avg:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae91f52",
   "metadata": {},
   "source": [
    "### Speed benchmark\n",
    "Does it actually speed up something? Yep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bf9472e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:08.033103Z",
     "end_time": "2023-03-14T00:57:11.958074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.30716948211193085 ms\n"
     ]
    }
   ],
   "source": [
    "elapsed = 0\n",
    "model = per_channel_quantized_model\n",
    "model.eval()\n",
    "num_batches = 100\n",
    "# Run the scripted model on a few batches of images\n",
    "for i, (images, target) in enumerate(data_loader_test):\n",
    "    if i < num_batches:\n",
    "        start = time()\n",
    "        output = model(images)\n",
    "        end = time()\n",
    "        elapsed = elapsed + (end - start)\n",
    "    else:\n",
    "        break\n",
    "num_images = images.size()[0] * num_batches\n",
    "\n",
    "print(f\"Elapsed time: {(elapsed / num_images * 1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "407c5d81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:11.963709Z",
     "end_time": "2023-03-14T00:57:16.458810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.3881186619400978 ms\n"
     ]
    }
   ],
   "source": [
    "elapsed = 0\n",
    "model = float_model\n",
    "model.eval()\n",
    "num_batches = 100\n",
    "# Run the scripted model on a few batches of images\n",
    "for i, (images, target) in enumerate(data_loader_test):\n",
    "    if i < num_batches:\n",
    "        start = time()\n",
    "        output = model(images)\n",
    "        end = time()\n",
    "        elapsed = elapsed + (end - start)\n",
    "    else:\n",
    "        break\n",
    "num_images = images.size()[0] * num_batches\n",
    "\n",
    "print(f\"Elapsed time: {(elapsed / num_images * 1000)} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6d7ad",
   "metadata": {},
   "source": [
    "## 45 GB T5 to a single GPU\n",
    "[Source](https://huggingface.co/blog/hf-bitsandbytes-integration) of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6c26c41",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:16.458156Z",
     "end_time": "2023-03-14T00:57:16.699838Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d07f8ec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:16.704226Z",
     "end_time": "2023-03-14T00:57:33.173057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 111\n",
      "CUDA SETUP: Loading binary /home/ubuntu/anaconda3/envs/ml/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda111_nocublaslt.so...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "394adc2ef8a34850bbf25390571beb59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"t5-3b-sharded\"  # @param [\"t5-11b-sharded\", \"t5-3b-sharded\"]\n",
    "\n",
    "# T5-3b and T5-11B are supported!\n",
    "# We need sharded weights otherwise we get CPU OOM errors\n",
    "model_id = f\"ybelkada/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_8bit = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b15c0d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:33.178138Z",
     "end_time": "2023-03-14T00:57:33.183463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5.300543488"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_8bit.get_memory_footprint() / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59d870",
   "metadata": {},
   "source": [
    "For t5-3b the int8 model is about ~5.3GB! whereas the original model has 11GB. For t5-11b the int8 model is about ~11GB vs 42GB for the original model. Now let's generate and see the qualitative results of the 8bit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a192201c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-14T00:57:33.196795Z",
     "end_time": "2023-03-14T00:57:37.332914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo mein Name ist Younes und ich bin ein Ingenieur für Machine Learning bei Hugging Face\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 50\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"translate English to German: Hello my name is Younes and I am a Machine Learning Engineer at Hugging Face\",\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "\n",
    "outputs = model_8bit.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@3.73.116.207 Frankfurt V100",
   "language": "",
   "name": "rik_ssh_ubuntu_3_73_116_207_frankfurtv100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
